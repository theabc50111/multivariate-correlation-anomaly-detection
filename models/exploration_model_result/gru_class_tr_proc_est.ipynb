{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc7f2c-2f0c-4729-b010-970bbfedaf83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import json\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"/workspace/multivariate-correlation-anomaly-detection/\")\n",
    "from utils.plot_utils import plot_gru_tr_process\n",
    "from utils.log_utils import Log\n",
    "\n",
    "JPY_LOGGER = Log(df_max_rows=50).init_logger(logger_name=\"ywt_jupyter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f34bb1-5524-4509-af4c-c8dc0123bad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_type(x, type_info):\n",
    "    return isinstance(x, type_info)\n",
    "\n",
    "\n",
    "def gru_class_tr_proc_est(log_path_list: list, condition_dict: dict, regexp_res: bool = False,  plot_pic:bool = True):\n",
    "    try:\n",
    "        df = pd.DataFrame()\n",
    "        for log_path in log_path_list:\n",
    "            with open(log_path, \"r\") as source:\n",
    "                log_dict = json.load(source)\n",
    "\n",
    "            for k in log_dict.keys():\n",
    "                locals()[k] = log_dict[k]\n",
    "            corr_info = str(next(filter(lambda p: p.startswith(\"corr\"), log_path.parts)))\n",
    "            min_tr_loss = min(locals()[\"tr_loss_history\"])\n",
    "            min_val_loss = min(locals()[\"val_loss_history\"])\n",
    "            max_tr_edge_acc = max(locals()[\"tr_edge_acc_history\"])\n",
    "            max_val_edge_acc = max(locals()[\"val_edge_acc_history\"])\n",
    "            model_struct_str = log_dict.get('model_structure')\n",
    "            record_fields = list(log_dict.keys()) + [\"corr_info\", \"min_tr_loss\", \"min_val_loss\", \"max_tr_edge_acc\", \"max_val_edge_acc\"]\n",
    "            assert not(set(condition_dict.keys()) - set(record_fields)), \"one of condition_dict.keys() doesn't match the record_fields if mts_corr_ad_est()\"\n",
    "            est_values_dict = {k:v for k, v in locals().items() if k in record_fields}\n",
    "            filtered_dict = dict(filter(lambda x: est_values_dict[x[0]] == x[1], condition_dict.items()))\n",
    "            if filtered_dict == condition_dict:\n",
    "                main_title_str = (f\"{locals().get('corr_info')} with batch_size({locals().get('batch_size')}) input to\\n\"\n",
    "                                  f\"GRU with gru_l{locals().get('gru_l')}-gru_h{locals().get('gru_h')} \"\n",
    "                                  f\"and drop: {locals().get('drop_p')} and loss_fns:{locals().get('loss_fns')}\\n\"\n",
    "                                  f\"min val-loss:{locals().get('min_val_loss'):8f} min tr-loss:{locals().get('min_tr_loss'):8f}\")\n",
    "                JPY_LOGGER.info(f\"file_name:{log_path.parts[-1]}\")\n",
    "                JPY_LOGGER.info(f\"file_path:{log_path.parts[2:-2]}\")\n",
    "                JPY_LOGGER.info(f\"main_title_str:\\n{main_title_str}\")\n",
    "                JPY_LOGGER.info(\"=\"*30)\n",
    "                comparison_dict = dict(filter(lambda x: x[0] in record_fields, locals().items()))\n",
    "                df = pd.concat([df, pd.DataFrame([comparison_dict])])\n",
    "                if plot_pic:\n",
    "                    plot_gru_tr_process(main_title=main_title_str, model_struct=model_struct_str, metrics_history={k:log_dict[k] for k in record_fields if \"history\" in k},\n",
    "                                        best_epoch=locals()['best_val_epoch'])\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            df = df.reindex([\"corr_info\", \"num_train_data\", \"num_val_data\", \"seq_len\", \"epochs\", \"batch_size\", \"tr_batches_per_epoch\", \"val_batches_per_epoch\", \"model_input_cus_bins\", \"input_feature_idx\",\n",
    "                             \"loss_fns\", \"custom_indices_loss_indices\", \"loss_weight\",\"opt_lr\", \"opt_weight_decay\", \"optimizer\", \"opt_scheduler\", \"metric_fn\", \"custom_indices_metric_indices\", \"drop_pos\", \"drop_p\", \"gru_l\",\n",
    "                             \"gru_h\", \"output_type\", \"target_data_bins\", \"tol_edge_acc_loss_atol\", 'best_val_epoch', \"min_tr_loss\", \n",
    "                             \"max_tr_edge_acc\", \"min_val_loss\", \"max_val_edge_acc\"], axis=1)\n",
    "            columns_containing_lists = df.where(np.vectorize(partial(check_type, type_info=list))).dropna(thresh=1, axis=1).columns\n",
    "            columns_containing_dicts = df.where(np.vectorize(partial(check_type, type_info=dict))).dropna(thresh=1, axis=1).columns\n",
    "            columns_containing_unhashable = columns_containing_lists.append(columns_containing_dicts)\n",
    "            columns_containing_hashable = df.columns.difference(columns_containing_unhashable)\n",
    "            independent_variables_columns = df.loc[::, columns_containing_hashable].nunique()[df.loc[::, columns_containing_hashable].nunique() > 1].index\n",
    "            control_variables_columns = df.loc[::, columns_containing_hashable].nunique().index.difference(independent_variables_columns)\n",
    "            for col in df.loc[::, columns_containing_unhashable]:\n",
    "                col_ser = df.loc[::, col].apply(lambda x:str(x))\n",
    "                df.loc[::, col] = col_ser\n",
    "                if len(np.unique(col_ser)) > 1:\n",
    "                    independent_variables_columns = independent_variables_columns.append(pd.Index([col]))\n",
    "                else:\n",
    "                    control_variables_columns = control_variables_columns.append(pd.Index([col]))\n",
    "            df = df.sort_values([\"batch_size\", \"seq_len\", \"gru_l\", \"gru_h\", \"drop_p\"], ascending=False)\n",
    "            df = df.sort_values([\"input_feature_idx\"], ascending=True)\n",
    "            model_tr_summary_df = df.reset_index(drop=True)\n",
    "            independent_variables_tr_summary_df = model_tr_summary_df.loc[::, independent_variables_columns].sort_index(axis=1)\n",
    "            control_variables_tr_summary_df = model_tr_summary_df.loc[0, control_variables_columns].sort_index(axis=0)\n",
    "            model_tr_summary_df.style.set_caption('Info of GRU_CLASS model with different hyperparameters')\n",
    "            pd.options.display.float_format = '{:.6f}'.format\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.max_colwidth', 80)\n",
    "            display(model_tr_summary_df)\n",
    "            display(independent_variables_tr_summary_df)\n",
    "            display(control_variables_tr_summary_df)\n",
    "    except Exception as e:\n",
    "        error_class = e.__class__.__name__ #⬞取得錯誤類型\n",
    "        detail = e.args[0]  #⬞取得詳細內容\n",
    "        cl, exc, tb = sys.exc_info() #⬞取得Call⬞Stack\n",
    "        last_call_stack = traceback.extract_tb(tb)[-1] #⬞取得Call⬞Stack的最後一筆資料↵\n",
    "        file_name = last_call_stack[0] #⬞取得發生的檔案名稱↵\n",
    "        line_num = last_call_stack[1] #⬞取得發生的行號↵\n",
    "        func_name = last_call_stack[2] #⬞取得發生的函數名稱\n",
    "        err_msg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(file_name, line_num, func_name, error_class, detail)\n",
    "        JPY_LOGGER.error(f\"file:{log_path.parts[-1]}, path:{log_path}\")\n",
    "        JPY_LOGGER.error(f\"===\\n{err_msg}\")\n",
    "        JPY_LOGGER.error(f\"===\\n{traceback.extract_tb(tb)}\")\n",
    "\n",
    "    return model_tr_summary_df, independent_variables_tr_summary_df, control_variables_tr_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ad5f2-ced5-4ca7-97db-0eeb2163880f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseline_gru_log_dir = Path(\"../save_models/gru_corr_class_custom_features/sp500_20112015-train_train/pearson/\")\n",
    "log_path_list1 = baseline_gru_log_dir.glob(\"./*[!deprecated][!archive][!.ipynb_checkpoints]*/train_logs/*[!.ipynb_checkpoints]*[.json]\")\n",
    "model_tr_summary_df, independent_variables_tr_summary_df, control_variables_tr_summary_df = gru_class_tr_proc_est(log_path_list=log_path_list1, condition_dict={}, plot_pic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ebaca-c67b-40ff-a94d-b2cbaf40243d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_independent_variables_tr_summary_df1 = independent_variables_tr_summary_df.where(independent_variables_tr_summary_df['max_val_edge_acc']<0.6).dropna()\n",
    "display(filtered_independent_variables_tr_summary_df1)\n",
    "filtered_independent_variables_tr_summary_df2 = independent_variables_tr_summary_df.where(independent_variables_tr_summary_df['input_feature_idx']=='[158]').dropna()\n",
    "display(filtered_independent_variables_tr_summary_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240cb47-0929-4854-8968-fbd6ba87b56f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
